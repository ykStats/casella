[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Casella-Statistical Inference",
    "section": "",
    "text": "StatsCentral\nCasella-Statistical Inference\n\nProbability 1\nProbability 2\nExpectations\nDistributions\n\nMultiple Random Variables\nRandom Samples\nData Reduction\nPoint Estimation\nHypothesis Testing\nInterval Estimation\nAsymptotic Evaluations\nANOVA and Regression\nLogistic Regression",
    "crumbs": [
      "Casella"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Definition 1.1.2 Let A, B be events, subsets of S, then:\nUnion: The union of A and B, written \\(A \\cup B\\) is the set of elements that belong to either A or B or both: \\[ A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\} \\] Intersection: The intersection of A and B, written \\(A \\cap B\\) is the set of elements that belong to both A and B: \\[ A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\} \\] Complement: The complement of A, written as \\(A^c\\), is the set of all elements that are not in A: \\[ A^c = \\{x: x \\notin A \\} \\]\n\n\nTheorem 1.1.4 For any three events, A, B and C, defined on a sample space S,\n\\[\\begin{array}{l l}\na. Commutativity    & A \\cup B = B \\cup A,\\\\\n                        & A \\cap B = B \\cap A;\\\\\nb. Associativity    & A \\cup (B \\cup C) = (A \\cup B)\\cup C,\\\\\n                        & $A \\cap (B \\cap C) = (A \\cap B)\\cap C;\\\\\nc. Distributive Laws    & A \\cap (B \\cup C) = (A \\cap B)\\cup (A \\cap C),\\\\\n                        & $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C);\\\\\nd. DeMorgan's Laws & (A \\cup B)^c = A^c \\cap B^c ,\\\\\n                   & (A \\cap B)^c = A^c \\cup B^c .\n\n\\end{array}\\]\n\n\n\n\nDefinition If \\(A_1, A_2, A_3, \\ldots\\) is a collection of sets, all defined on a sample space \\(S\\), then\n\\[\\begin{align*}\n\\bigcup_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for some }i\\},\\\\\n\\bigcap_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for all }i\\}.\\\\\n\\end{align*}\\]\n\n\n\nDefinition 1.1.5 Two events \\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if \\(A \\cap B = 0\\). The events \\(A_1,A_2,\\ldots\\) are pairwise disjoint (or mutually exclusive) if \\(A_i \\cap A_j = 0 \\text{ for all } i \\ne j\\).\n\n\n\nDefinition 1.1.6 If \\(A_1,A_2,\\ldots\\) are pairwise disjoint and \\(\\bigcup_{i=0}^\\infty A_i = S\\), then the collection \\(A_1, A_2, \\dots\\) forms a partition of S.\n\n\n\n\n\nDefinition 1.2.1 A collection of subsets of S is called a sigma algebra (or Borel field ), denoted by \\(\\mathscr{B}\\), if it satifies the following three properties:\n\n\\(0 \\in \\mathscr{B}\\) (the empty set is an element of \\(\\mathscr{B}\\))\nIf \\(A \\in \\mathscr{B}\\) then \\(A^c \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under complementation)\nIf \\(A_1, A_2, \\ldots \\in \\mathscr{B}\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under countable unions)."
  },
  {
    "objectID": "prob.html#sets",
    "href": "prob.html#sets",
    "title": "Probability Theory",
    "section": "",
    "text": "Definition 1.1.2 Let A, B be events, subsets of S, then:\nUnion: The union of A and B, written \\(A \\cup B\\) is the set of elements that belong to either A or B or both: \\[ A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\} \\] Intersection: The intersection of A and B, written \\(A \\cap B\\) is the set of elements that belong to both A and B: \\[ A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\} \\] Complement: The complement of A, written as \\(A^c\\), is the set of all elements that are not in A: \\[ A^c = \\{x: x \\notin A \\} \\]\n\n\nTheorem 1.1.4 For any three events, A, B and C, defined on a sample space S,\n\\[\\begin{array}{l l}\na. Commutativity    & A \\cup B = B \\cup A,\\\\\n                        & A \\cap B = B \\cap A;\\\\\nb. Associativity    & A \\cup (B \\cup C) = (A \\cup B)\\cup C,\\\\\n                        & $A \\cap (B \\cap C) = (A \\cap B)\\cap C;\\\\\nc. Distributive Laws    & A \\cap (B \\cup C) = (A \\cap B)\\cup (A \\cap C),\\\\\n                        & $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C);\\\\\nd. DeMorgan's Laws & (A \\cup B)^c = A^c \\cap B^c ,\\\\\n                   & (A \\cap B)^c = A^c \\cup B^c .\n\n\\end{array}\\]\n\n\n\n\nDefinition If \\(A_1, A_2, A_3, \\ldots\\) is a collection of sets, all defined on a sample space \\(S\\), then\n\\[\\begin{align*}\n\\bigcup_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for some }i\\},\\\\\n\\bigcap_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for all }i\\}.\\\\\n\\end{align*}\\]\n\n\n\nDefinition 1.1.5 Two events \\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if \\(A \\cap B = 0\\). The events \\(A_1,A_2,\\ldots\\) are pairwise disjoint (or mutually exclusive) if \\(A_i \\cap A_j = 0 \\text{ for all } i \\ne j\\).\n\n\n\nDefinition 1.1.6 If \\(A_1,A_2,\\ldots\\) are pairwise disjoint and \\(\\bigcup_{i=0}^\\infty A_i = S\\), then the collection \\(A_1, A_2, \\dots\\) forms a partition of S.\n\n\n\n\n\nDefinition 1.2.1 A collection of subsets of S is called a sigma algebra (or Borel field ), denoted by \\(\\mathscr{B}\\), if it satifies the following three properties:\n\n\\(0 \\in \\mathscr{B}\\) (the empty set is an element of \\(\\mathscr{B}\\))\nIf \\(A \\in \\mathscr{B}\\) then \\(A^c \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under complementation)\nIf \\(A_1, A_2, \\ldots \\in \\mathscr{B}\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under countable unions)."
  },
  {
    "objectID": "prob.html#sigma-algebra",
    "href": "prob.html#sigma-algebra",
    "title": "Chap 01: Probability Theory",
    "section": "2 Sigma-Algebra",
    "text": "2 Sigma-Algebra",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "prob.html#section",
    "href": "prob.html#section",
    "title": "Chap 01: Probability Theory",
    "section": "2 ",
    "text": "2 \n\nTheorem 1.2.6 Let \\(S = {s_1,\\ldots,s_n}\\) be a finite set. Let \\(\\mathscr{B}\\) be any sigma algebra$",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "prob.html#counting-probabilities",
    "href": "prob.html#counting-probabilities",
    "title": "Chap 01: Probability Theory",
    "section": "Counting Probabilities",
    "text": "Counting Probabilities"
  },
  {
    "objectID": "prob.html#counting-and-probabilities",
    "href": "prob.html#counting-and-probabilities",
    "title": "Chap 01: Probability Theory",
    "section": "Counting and Probabilities",
    "text": "Counting and Probabilities\n\nDefinition 1.2.17 For nonnegative integers \\(n\\) and \\(r\\), where \\(n \\ge r\\), we define the symbol \\(n \\choose r\\), read n choose r, as \\[\n  {n \\choose r} = \\frac{n!}{r!(n-r)!}\n\\]"
  },
  {
    "objectID": "prob.html#counting-prob",
    "href": "prob.html#counting-prob",
    "title": "Chap 01: Probability Theory",
    "section": "Counting Prob",
    "text": "Counting Prob\n\nDefinition 1.2.17 For nonnegative integers \\(n\\) and \\(r\\), where \\(n \\ge r\\), we define the symbol \\(n \\choose r\\), read n choose r, as \\[\n  {n \\choose r} = \\frac{n!}{r!(n-r)!}\n\\]"
  },
  {
    "objectID": "prob.html#probabilities",
    "href": "prob.html#probabilities",
    "title": "Probability Theory",
    "section": "Probabilities",
    "text": "Probabilities\n\nKolmogorov Axioms\n\nDefinition 1.2.4 Given a sample space \\(S\\) and an associated sigma algebra \\(\\mathscr{B}\\) that satisfies\n\n\\(P(A) \\ge 0\\) for all \\(A \\in \\mathscr{B}\\)\n\\(P(S) = 1\\)\nIf \\(A_1,A_2,\\ldots \\in \\mathscr{B}\\) are pairwise disjoint, then \\(P(\\bigcup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty P(A)\\)\n\n\n\n\n\nProbability Function\n\nTheorem 1.2.6 Let \\(S = {s_1,\\ldots,s_n}\\) be a finite set. Let \\(\\mathscr{B}\\) be any sigma algebra of subsets of \\(S\\). Let \\(p_1,\\ldots,p_n\\) be nonnegative numbers that sum to 1. For any \\(A \\in \\mathscr{B}\\), define \\(P(A)\\) by \\[\nP(A) = \\sum_{\\{i:s_i \\in A\\}} p_i\n\\] (The sum over an empty set is defined to be \\(0\\).) Then \\(P\\) is a probability function on \\(\\mathscr{B}\\). This remains true if \\(S={s_1, s_2, \\ldots }\\) is a countable set.\n\n\n\nTheorem 1.2.8 If \\(P\\) is a probability function and \\(A\\) is any set in \\(\\mathscr{B}\\), then\n\n\\(P(0) = 0\\) where \\(0\\) is the empty set;\n\\(P(A) \\le 1\\);\n\\(P(A^c) = 1 - P(A)\\).\n\n\n\n\nTheorem 1.2.9 If \\(P\\) is a probability function and \\(A\\) and \\(B\\) are any sets in \\(\\mathscr{B}\\) then\n\n\\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\);\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\);\nIF \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\nTheorem 1.2.11 If \\(P\\) is a probability function, then\n\n\\(P(A) = \\sum_{i=1}^\\infty P(A \\cap C_i )\\) for any partition \\(C_1, C_2, \\ldots ;\\)\n\\(P(\\cup_{i=1}^\\infty A_i )\\le \\sum_{i=1}^\\infty P(A_i)\\) for any sets $A_1, A_2, \\ldots $;\n\n\n\n\nCounting\n\nDefinition 1.2.17 For nonnegative integers \\(n\\) and \\(r\\), where \\(n \\ge r\\), we define the symbol \\(n \\choose r\\), read n choose r, as \\[\n{n \\choose r} = \\dfrac{n!}{r!(n-r)!}\n\\]\n\n\n\nWith Replacements\nTable 1.2.1 Number of possible arrangements of size \\(r\\) from \\(n\\) objects.\n\\[\\begin{array}{c c c}\n\\hline\n          & {Without}  & {With}  \\\\\n          & Replacement & Replacement\\\\\n\\hline\n  Ordered & \\dfrac{n!}{(n-r)!} & n^r \\\\\n  Unordered & \\displaystyle{ n \\choose r}& \\displaystyle{{n+r-1} \\choose r}\\\\\n\\hline\n\\end{array}\\]\n\nExample: drawing five cards (Poker) from a deck of 52 cards (i.e. sampling without replacement).\n\n\nUnordered arrangement of five cards = \\(\\displaystyle {52 \\choose 5}\\) = \\(\\dfrac{52!}{47!5!}\\) = 2,598,960 \\(\\qquad \\blacksquare\\)\n\n\nFour aces out of 5 cards\n\nthere are 48 (52-4) different ways to choose fifth card. \\[\nP (\\text{four aces}) = \\dfrac{48}{2,598,960} = {1}:{254,145} \\qquad \\blacksquare\n\\]\n\nFour of a kind e.g. four queens\n\nthere are 13 ways to specify which denomination {1:King}.\nthere are 48 ways to select the fifth card. \\[\nP(\\text{four of a kind}) = \\dfrac{(13)(48)}{2,598,960} = \\dfrac{624}{2,598,960}= {1}:{4,165} \\qquad \\blacksquare\n\\]\n\nExactly one pair only e.g. queen pair\n\noutcomes for one pair only :\nspecify denomination for the pair\ntwo cards from a denomination\nspecify the other three denonimations\nspecify the other three cards from the denominations\n\n\n\\[\n  13 {4 \\choose 2} {12 \\choose 3} 4^3 = 1,098,240\n\\]\nThus,\n\\[\nP(\\text{exactly one pair})=\\dfrac{1,098,240}{2,598,960} = 1:(2.58) \\qquad\\blacksquare\n\\]\n\n\nConditional Probability\n\nDefinition 1.3.2 If \\(A\\) and \\(B\\) are events in \\(S\\) and \\(P(B) \\gt 0\\) , then the conditional probability of \\(A\\) given \\(B\\) , written as \\(P(A|B)\\) is:\n\\[\nP(A|B) = \\dfrac{P(A \\cap B}{P(B)}\n\\]\n\n\\(\\\\\\)\n\nTheorem 1.3.5 (Bayes’ Rule) Let \\(A_1,A_2, \\ldots\\) be a partition of the sample space, and let B be any set. Then for each i=1,2,, \\[\nP(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^\\infty P(B|A_j)P(A_j)}\n\\]\n\n\\(\\\\\\)\n\nDefinition 1.3.7 Two events \\(A\\) and \\(B\\) are statistically independent if:\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\n\n\\(\\\\\\)\n\nDefinition 1.3.12 A collection of events \\(A_1, \\ldots, A_n\\) are mutually independent if for any subcollection \\(A_{i_1},\\dots,A_{i_k}\\), we have\n\\[\nP(\\bigcap_{j=1}^k A_{i_j}) = \\prod_{j=1}^k P(A_{i_j}).\n\\]"
  },
  {
    "objectID": "prob.html#counting",
    "href": "prob.html#counting",
    "title": "Chap 01: Probability Theory",
    "section": "Counting",
    "text": "Counting\n\nDefinition 1.2.17 For nonnegative integers \\(n\\) and \\(r\\), where \\(n \\ge r\\), we define the symbol \\(n \\choose r\\), read n choose r, as \\[\n  {n \\choose r} = \\frac{n!}{r!(n-r)!}\n\\]"
  },
  {
    "objectID": "prob.html#without-replacement-with-replacement",
    "href": "prob.html#without-replacement-with-replacement",
    "title": "Chap 01: Probability Theory",
    "section": "| |Without replacement| With replacement|",
    "text": "| |Without replacement| With replacement|\n|Ordered| a |b | |Unordered| c| d| —\n\n\n\n\nWithout replacement\nWith replacement\n\n\n\n\nOrdered\n\\(\\frac{n!}{(n-r)!}\\)\nn^r\n\n\nUnordered\n\\(n \\choose r\\)\n\\({n+r-1} \\choose r\\)"
  },
  {
    "objectID": "randvar.html",
    "href": "randvar.html",
    "title": "Random Variables",
    "section": "",
    "text": "Definition 1.4.1 A random variable is a function from a sample space \\(S\\) into the real numbers.\n\n\nSuppose \\(S = \\{s_1,\\ldots, s_n\\}\\) and there is a probability function \\(P\\) on random variable \\(X\\) with range=\\(\\{x_1,\\ldots,x_m\\}\\), then,\n\\[\nP(X = x_i) = P({s_j \\in S: X(s_j)= x_i})\n\\]\n\nProb Function on a Random Variable\nExample 1.4.3 (Three coin tosses-II)\nConsider experiment of tossing a fair coin three times. Define the random variable \\(X\\) to be the number of heads obtained in three tosses.\nThe complete enumeration of X for each point in the sample space is\n\\[\\begin{array}{c c c c c c c c c}\n\\hline\n  s & HHH & HHT & HTH & THH & TTH & THT & HTT & TTT\\\\\n\\hline\nX(s) & 3 & 2 & 2 & 2 & 1 & 1 & 1 & 0 \\\\\n\\hline\n\\end{array}\\]\nThe range for the random variable \\(X\\) is \\({\\it{X}}=\\{0,1,2,3\\}\\).\nThe probability function is:\n\\[\\begin{array}{c c c c c}\n\\hline\n  x & 0 & 1 & 2 & 3 \\\\\n\\hline\nP(X=x) & \\frac{1}{8} & \\frac{3}{8} & \\frac{3}{8} & \\frac{1}{8} \\\\\n\\hline\n\\end{array}\\]\nFor example, \\(P(X=1) = P({HTT, THT, TTH}) = \\frac{3}{8}\\)\n\n\nCumulative Distribution Functions\n\nDefinition 1.5.1 the cumulative distribution function or cdf of a random variable \\(X\\), is defined by:\n\\[\nF(X) = P(X \\le x), \\text{ for all } x\n\\]\n\n\n\nExample 1.5.2 (Three coin tosses-III) Experiment of tossing three fair coins. Let \\(X\\) be the number of heads observed. The \\(cdf\\) of \\(X\\) is\n\\[\\begin{align*}\n  F(x) =\n  \\begin{dcases*}\n    0 & if  $-\\infty &lt; x &lt; 0$\\\\\n    \\tfrac{1}{8} & if  $0 \\le x \\lt 1 $\\\\\n    \\tfrac{1}{2} & if  $1 \\le x \\lt 2 $\\\\\n    \\tfrac{7}{8} & if  $2 \\le x \\lt 3 $\\\\\n    1 & if  $3 \\le x \\lt \\infty $\\\\\n  \\end{dcases*}\n\\end{align*}\\]\n\n\nR-Code for discrete CDF\n\nlibrary(DiscreteDists)\nx &lt;- 0:3\nfx &lt;- c(0.125,0.375,0.375,0.125)\nplot_discrete_cdf(x, fx, las=1, main=\"Discrete Cumulative Distribution Function (cdf)\")"
  },
  {
    "objectID": "randvar.html#sets",
    "href": "randvar.html#sets",
    "title": "Probability Theory",
    "section": "",
    "text": "Definition 1.1.2 Let A, B be events, subsets of S, then:\nUnion: The union of A and B, written \\(A \\cup B\\) is the set of elements that belong to either A or B or both: \\[ A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\} \\] Intersection: The intersection of A and B, written \\(A \\cap B\\) is the set of elements that belong to both A and B: \\[ A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\} \\] Complement: The complement of A, written as \\(A^c\\), is the set of all elements that are not in A: \\[ A^c = \\{x: x \\notin A \\} \\]\n\n\nTheorem 1.1.4 For any three events, A, B and C, defined on a sample space S,\n\\[\\begin{array}{l l}\na. Commutativity    & A \\cup B = B \\cup A,\\\\\n                        & A \\cap B = B \\cap A;\\\\\nb. Associativity    & A \\cup (B \\cup C) = (A \\cup B)\\cup C,\\\\\n                        & $A \\cap (B \\cap C) = (A \\cap B)\\cap C;\\\\\nc. Distributive Laws    & A \\cap (B \\cup C) = (A \\cap B)\\cup (A \\cap C),\\\\\n                        & $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C);\\\\\nd. DeMorgan's Laws & (A \\cup B)^c = A^c \\cap B^c ,\\\\\n                   & (A \\cap B)^c = A^c \\cup B^c .\n\n\\end{array}\\]\n\n\n\n\nDefinition If \\(A_1, A_2, A_3, \\ldots\\) is a collection of sets, all defined on a sample space \\(S\\), then\n\\[\\begin{align*}\n\\bigcup_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for some }i\\},\\\\\n\\bigcap_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for all }i\\}.\\\\\n\\end{align*}\\]\n\n\n\nDefinition 1.1.5 Two events \\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if \\(A \\cap B = 0\\). The events \\(A_1,A_2,\\ldots\\) are pairwise disjoint (or mutually exclusive) if \\(A_i \\cap A_j = 0 \\text{ for all } i \\ne j\\).\n\n\n\nDefinition 1.1.6 If \\(A_1,A_2,\\ldots\\) are pairwise disjoint and \\(\\bigcup_{i=0}^\\infty A_i = S\\), then the collection \\(A_1, A_2, \\dots\\) forms a partition of S.\n\n\n\n\n\nDefinition 1.2.1 A collection of subsets of S is called a sigma algebra (or Borel field ), denoted by \\(\\mathscr{B}\\), if it satifies the following three properties:\n\n\\(0 \\in \\mathscr{B}\\) (the empty set is an element of \\(\\mathscr{B}\\))\nIf \\(A \\in \\mathscr{B}\\) then \\(A^c \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under complementation)\nIf \\(A_1, A_2, \\ldots \\in \\mathscr{B}\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under countable unions)."
  },
  {
    "objectID": "randvar.html#probabilities",
    "href": "randvar.html#probabilities",
    "title": "Probability Theory",
    "section": "Probabilities",
    "text": "Probabilities\n\nKolmogorov Axioms\n\nDefinition 1.2.4 Given a sample space \\(S\\) and an associated sigma algebra \\(\\mathscr{B}\\) that satisfies\n\n\\(P(A) \\ge 0\\) for all \\(A \\in \\mathscr{B}\\)\n\\(P(S) = 1\\)\nIf \\(A_1,A_2,\\ldots \\in \\mathscr{B}\\) are pairwise disjoint, then \\(P(\\bigcup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty P(A)\\)\n\n\n\n\n\nProbability Function\n\nTheorem 1.2.6 Let \\(S = {s_1,\\ldots,s_n}\\) be a finite set. Let \\(\\mathscr{B}\\) be any sigma algebra of subsets of \\(S\\). Let \\(p_1,\\ldots,p_n\\) be nonnegative numbers that sum to 1. For any \\(A \\in \\mathscr{B}\\), define \\(P(A)\\) by \\[\nP(A) = \\sum_{\\{i:s_i \\in A\\}} p_i\n\\] (The sum over an empty set is defined to be \\(0\\).) Then \\(P\\) is a probability function on \\(\\mathscr{B}\\). This remains true if \\(S={s_1, s_2, \\ldots }\\) is a countable set.\n\n\n\nTheorem 1.2.8 If \\(P\\) is a probability function and \\(A\\) is any set in \\(\\mathscr{B}\\), then\n\n\\(P(0) = 0\\) where \\(0\\) is the empty set;\n\\(P(A) \\le 1\\);\n\\(P(A^c) = 1 - P(A)\\).\n\n\n\n\nTheorem 1.2.9 If \\(P\\) is a probability function and \\(A\\) and \\(B\\) are any sets in \\(\\mathscr{B}\\) then\n\n\\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\);\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\);\nIF \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\nTheorem 1.2.11 If \\(P\\) is a probability function, then\n\n\\(P(A) = \\sum_{i=1}^\\infty P(A \\cap C_i )\\) for any partition \\(C_1, C_2, \\ldots ;\\)\n\\(P(\\cup_{i=1}^\\infty A_i )\\le \\sum_{i=1}^\\infty P(A_i)\\) for any sets $A_1, A_2, \\ldots $;\n\n\n\n\nCounting\n\nDefinition 1.2.17 For nonnegative integers \\(n\\) and \\(r\\), where \\(n \\ge r\\), we define the symbol \\(n \\choose r\\), read n choose r, as \\[\n{n \\choose r} = \\dfrac{n!}{r!(n-r)!}\n\\]\n\n\n\nWith Replacements\nTable 1.2.1 Number of possible arrangements of size \\(r\\) from \\(n\\) objects.\n\\[\\begin{array}{c c c}\n\\hline\n          & {Without}  & {With}  \\\\\n          & Replacement & Replacement\\\\\n\\hline\n  Ordered & \\dfrac{n!}{(n-r)!} & n^r \\\\\n  Unordered & \\displaystyle{ n \\choose r}& \\displaystyle{{n+r-1} \\choose r}\\\\\n\\hline\n\\end{array}\\]\n\nExample: drawing five cards (Poker) from a deck of 52 cards (i.e. sampling without replacement).\n\n\nUnordered arrangement of five cards = \\(\\displaystyle {52 \\choose 5}\\) = \\(\\dfrac{52!}{47!5!}\\) = 2,598,960 \\(\\qquad \\blacksquare\\)\n\n\nFour aces out of 5 cards\n\nthere are 48 (52-4) different ways to choose fifth card. \\[\nP (\\text{four aces}) = \\dfrac{48}{2,598,960} = {1}:{254,145} \\qquad \\blacksquare\n\\]\n\nFour of a kind e.g. four queens\n\nthere are 13 ways to specify which denomination {1:King}.\nthere are 48 ways to select the fifth card. \\[\nP(\\text{four of a kind}) = \\dfrac{(13)(48)}{2,598,960} = \\dfrac{624}{2,598,960}= {1}:{4,165} \\qquad \\blacksquare\n\\]\n\nExactly one pair only e.g. queen pair\n\noutcomes for one pair only :\nspecify denomination for the pair\ntwo cards from a denomination\nspecify the other three denonimations\nspecify the other three cards from the denominations\n\n\n\\[\n  13 {4 \\choose 2} {12 \\choose 3} 4^3 = 1,098,240\n\\]\nThus,\n\\[\nP(\\text{exactly one pair})=\\dfrac{1,098,240}{2,598,960} = 1:(2.58) \\qquad\\blacksquare\n\\]\n\n\nConditional Probability\n\nDefinition 1.3.2 If \\(A\\) and \\(B\\) are events in \\(S\\) and \\(P(B) \\gt 0\\) , then the conditional probability of \\(A\\) given \\(B\\) , written as \\(P(A|B)\\) is:\n\\[\nP(A|B) = \\dfrac{P(A \\cap B}{P(B)}\n\\]\n\n\\(\\\\\\)\n\nTheorem 1.3.5 (Bayes’ Rule) Let \\(A_1,A_2, \\ldots\\) be a partition of the sample space, and let B be any set. Then for each i=1,2,, \\[\nP(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^\\infty P(B|A_j)P(A_j)}\n\\]\n\n\\(\\\\\\)\n\nDefinition 1.3.7 Two events \\(A\\) and \\(B\\) are statistically independent if:\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\n\n\\(\\\\\\)\n\nDefinition 1.3.12 A collection of events \\(A_1, \\ldots, A_n\\) are mutually independent if for any subcollection \\(A_{i_1},\\dots,A_{i_k}\\), we have\n\\[\nP(\\bigcap_{j=1}^k A_{i_j}) = \\prod_{j=1}^k P(A_{i_j}).\n\\]"
  },
  {
    "objectID": "prob1.html",
    "href": "prob1.html",
    "title": "Probability Theory 1",
    "section": "",
    "text": "Definition 1.1.2 Let A, B be events, subsets of S, then:\nUnion: The union of A and B, written \\(A \\cup B\\) is the set of elements that belong to either A or B or both: \\[ A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\} \\] Intersection: The intersection of A and B, written \\(A \\cap B\\) is the set of elements that belong to both A and B: \\[ A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\} \\] Complement: The complement of A, written as \\(A^c\\), is the set of all elements that are not in A: \\[ A^c = \\{x: x \\notin A \\} \\]\n\n\nTheorem 1.1.4 For any three events, A, B and C, defined on a sample space S,\n\\[\\begin{array}{l l}\na. Commutativity    & A \\cup B = B \\cup A,\\\\\n                        & A \\cap B = B \\cap A;\\\\\nb. Associativity    & A \\cup (B \\cup C) = (A \\cup B)\\cup C,\\\\\n                        & $A \\cap (B \\cap C) = (A \\cap B)\\cap C;\\\\\nc. Distributive Laws    & A \\cap (B \\cup C) = (A \\cap B)\\cup (A \\cap C),\\\\\n                        & $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C);\\\\\nd. DeMorgan's Laws & (A \\cup B)^c = A^c \\cap B^c ,\\\\\n                   & (A \\cap B)^c = A^c \\cup B^c .\n\n\\end{array}\\]\n\n\n\n\nDefinition If \\(A_1, A_2, A_3, \\ldots\\) is a collection of sets, all defined on a sample space \\(S\\), then\n\\[\\begin{align*}\n\\bigcup_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for some }i\\},\\\\\n\\bigcap_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for all }i\\}.\\\\\n\\end{align*}\\]\n\n\n\nDefinition 1.1.5 Two events \\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if \\(A \\cap B = 0\\). The events \\(A_1,A_2,\\ldots\\) are pairwise disjoint (or mutually exclusive) if \\(A_i \\cap A_j = 0 \\text{ for all } i \\ne j\\).\n\n\n\nDefinition 1.1.6 If \\(A_1,A_2,\\ldots\\) are pairwise disjoint and \\(\\bigcup_{i=0}^\\infty A_i = S\\), then the collection \\(A_1, A_2, \\dots\\) forms a partition of S.\n\n\n\n\n\nDefinition 1.2.1 A collection of subsets of S is called a sigma algebra (or Borel field ), denoted by \\(\\mathscr{B}\\), if it satifies the following three properties:\n\n\\(0 \\in \\mathscr{B}\\) (the empty set is an element of \\(\\mathscr{B}\\))\nIf \\(A \\in \\mathscr{B}\\) then \\(A^c \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under complementation)\nIf \\(A_1, A_2, \\ldots \\in \\mathscr{B}\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under countable unions)."
  },
  {
    "objectID": "prob1.html#sets",
    "href": "prob1.html#sets",
    "title": "Probability Theory 1",
    "section": "",
    "text": "Definition 1.1.2 Let A, B be events, subsets of S, then:\nUnion: The union of A and B, written \\(A \\cup B\\) is the set of elements that belong to either A or B or both: \\[ A \\cup B = \\{x: x \\in A \\text{ or } x \\in B\\} \\] Intersection: The intersection of A and B, written \\(A \\cap B\\) is the set of elements that belong to both A and B: \\[ A \\cap B = \\{x: x \\in A \\text{ and } x \\in B\\} \\] Complement: The complement of A, written as \\(A^c\\), is the set of all elements that are not in A: \\[ A^c = \\{x: x \\notin A \\} \\]\n\n\nTheorem 1.1.4 For any three events, A, B and C, defined on a sample space S,\n\\[\\begin{array}{l l}\na. Commutativity    & A \\cup B = B \\cup A,\\\\\n                        & A \\cap B = B \\cap A;\\\\\nb. Associativity    & A \\cup (B \\cup C) = (A \\cup B)\\cup C,\\\\\n                        & $A \\cap (B \\cap C) = (A \\cap B)\\cap C;\\\\\nc. Distributive Laws    & A \\cap (B \\cup C) = (A \\cap B)\\cup (A \\cap C),\\\\\n                        & $A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C);\\\\\nd. DeMorgan's Laws & (A \\cup B)^c = A^c \\cap B^c ,\\\\\n                   & (A \\cap B)^c = A^c \\cup B^c .\n\n\\end{array}\\]\n\n\n\n\nDefinition If \\(A_1, A_2, A_3, \\ldots\\) is a collection of sets, all defined on a sample space \\(S\\), then\n\\[\\begin{align*}\n\\bigcup_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for some }i\\},\\\\\n\\bigcap_{i=1}^\\infty A_i &= \\{x \\in S: x \\in A_i  \\text{ for all }i\\}.\\\\\n\\end{align*}\\]\n\n\n\nDefinition 1.1.5 Two events \\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if \\(A \\cap B = 0\\). The events \\(A_1,A_2,\\ldots\\) are pairwise disjoint (or mutually exclusive) if \\(A_i \\cap A_j = 0 \\text{ for all } i \\ne j\\).\n\n\n\nDefinition 1.1.6 If \\(A_1,A_2,\\ldots\\) are pairwise disjoint and \\(\\bigcup_{i=0}^\\infty A_i = S\\), then the collection \\(A_1, A_2, \\dots\\) forms a partition of S.\n\n\n\n\n\nDefinition 1.2.1 A collection of subsets of S is called a sigma algebra (or Borel field ), denoted by \\(\\mathscr{B}\\), if it satifies the following three properties:\n\n\\(0 \\in \\mathscr{B}\\) (the empty set is an element of \\(\\mathscr{B}\\))\nIf \\(A \\in \\mathscr{B}\\) then \\(A^c \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under complementation)\nIf \\(A_1, A_2, \\ldots \\in \\mathscr{B}\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathscr{B}\\) (\\(\\mathscr{B}\\) is closed under countable unions)."
  },
  {
    "objectID": "prob1.html#probabilities",
    "href": "prob1.html#probabilities",
    "title": "Probability Theory 1",
    "section": "Probabilities",
    "text": "Probabilities\n\nKolmogorov Axioms\n\nDefinition 1.2.4 Given a sample space \\(S\\) and an associated sigma algebra \\(\\mathscr{B}\\) that satisfies\n\n\\(P(A) \\ge 0\\) for all \\(A \\in \\mathscr{B}\\)\n\\(P(S) = 1\\)\nIf \\(A_1,A_2,\\ldots \\in \\mathscr{B}\\) are pairwise disjoint, then \\(P(\\bigcup_{i=1}^\\infty A_i) = \\sum_{i=1}^\\infty P(A)\\)\n\n\n\n\n\nProbability Function\n\nTheorem 1.2.6 Let \\(S = {s_1,\\ldots,s_n}\\) be a finite set. Let \\(\\mathscr{B}\\) be any sigma algebra of subsets of \\(S\\). Let \\(p_1,\\ldots,p_n\\) be nonnegative numbers that sum to 1. For any \\(A \\in \\mathscr{B}\\), define \\(P(A)\\) by \\[\nP(A) = \\sum_{\\{i:s_i \\in A\\}} p_i\n\\] (The sum over an empty set is defined to be \\(0\\).) Then \\(P\\) is a probability function on \\(\\mathscr{B}\\). This remains true if \\(S={s_1, s_2, \\ldots }\\) is a countable set.\n\n\n\nTheorem 1.2.8 If \\(P\\) is a probability function and \\(A\\) is any set in \\(\\mathscr{B}\\), then\n\n\\(P(0) = 0\\) where \\(0\\) is the empty set;\n\\(P(A) \\le 1\\);\n\\(P(A^c) = 1 - P(A)\\).\n\n\n\n\nTheorem 1.2.9 If \\(P\\) is a probability function and \\(A\\) and \\(B\\) are any sets in \\(\\mathscr{B}\\) then\n\n\\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\);\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\);\nIF \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\nTheorem 1.2.11 If \\(P\\) is a probability function, then\n\n\\(P(A) = \\sum_{i=1}^\\infty P(A \\cap C_i )\\) for any partition \\(C_1, C_2, \\ldots ;\\)\n\\(P(\\cup_{i=1}^\\infty A_i )\\le \\sum_{i=1}^\\infty P(A_i)\\) for any sets $A_1, A_2, \\ldots $;\n\n\n\n\nCounting\n\nDefinition 1.2.17 For nonnegative integers \\(n\\) and \\(r\\), where \\(n \\ge r\\), we define the symbol \\(n \\choose r\\), read n choose r, as \\[\n{n \\choose r} = \\dfrac{n!}{r!(n-r)!}\n\\]\n\n\n\nWith Replacements\nTable 1.2.1 Number of possible arrangements of size \\(r\\) from \\(n\\) objects.\n\\[\\begin{array}{c c c}\n\\hline\n          & {Without}  & {With}  \\\\\n          & Replacement & Replacement\\\\\n\\hline\n  Ordered & \\dfrac{n!}{(n-r)!} & n^r \\\\\n  Unordered & \\displaystyle{ n \\choose r}& \\displaystyle{{n+r-1} \\choose r}\\\\\n\\hline\n\\end{array}\\]\n\nExample: drawing five cards (Poker) from a deck of 52 cards (i.e. sampling without replacement).\n\n\nUnordered arrangement of five cards = \\(\\displaystyle {52 \\choose 5}\\) = \\(\\dfrac{52!}{47!5!}\\) = 2,598,960 \\(\\qquad \\blacksquare\\)\n\n\nFour aces out of 5 cards\n\nthere are 48 (52-4) different ways to choose fifth card. \\[\nP (\\text{four aces}) = \\dfrac{48}{2,598,960} = {1}:{254,145} \\qquad \\blacksquare\n\\]\n\nFour of a kind e.g. four queens\n\nthere are 13 ways to specify which denomination {1:King}.\nthere are 48 ways to select the fifth card. \\[\nP(\\text{four of a kind}) = \\dfrac{(13)(48)}{2,598,960} = \\dfrac{624}{2,598,960}= {1}:{4,165} \\qquad \\blacksquare\n\\]\n\nExactly one pair only e.g. queen pair\n\noutcomes for one pair only :\nspecify denomination for the pair\ntwo cards from a denomination\nspecify the other three denonimations\nspecify the other three cards from the denominations\n\n\n\\[\n  13 {4 \\choose 2} {12 \\choose 3} 4^3 = 1,098,240\n\\]\nThus,\n\\[\nP(\\text{exactly one pair})=\\dfrac{1,098,240}{2,598,960} = 1:(2.58) \\qquad\\blacksquare\n\\]\n\n\nConditional Probability\n\nDefinition 1.3.2 If \\(A\\) and \\(B\\) are events in \\(S\\) and \\(P(B) \\gt 0\\) , then the conditional probability of \\(A\\) given \\(B\\) , written as \\(P(A|B)\\) is:\n\\[\nP(A|B) = \\dfrac{P(A \\cap B}{P(B)}\n\\]\n\n\\(\\\\\\)\n\nTheorem 1.3.5 (Bayes’ Rule) Let \\(A_1,A_2, \\ldots\\) be a partition of the sample space, and let B be any set. Then for each i=1,2,, \\[\nP(A_i|B) = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^\\infty P(B|A_j)P(A_j)}\n\\]\n\n\\(\\\\\\)\n\nDefinition 1.3.7 Two events \\(A\\) and \\(B\\) are statistically independent if:\n\\[\nP(A \\cap B) = P(A)P(B)\n\\]\n\n\\(\\\\\\)\n\nDefinition 1.3.12 A collection of events \\(A_1, \\ldots, A_n\\) are mutually independent if for any subcollection \\(A_{i_1},\\dots,A_{i_k}\\), we have\n\\[\nP(\\bigcap_{j=1}^k A_{i_j}) = \\prod_{j=1}^k P(A_{i_j}).\n\\]"
  },
  {
    "objectID": "prob2.html",
    "href": "prob2.html",
    "title": "Probability Theory 2",
    "section": "",
    "text": "Definition 1.4.1 A random variable is a function from a sample space \\(S\\) into the real numbers.\n\n\nSuppose \\(S = \\{s_1,\\ldots, s_n\\}\\) and there is a probability function \\(P\\) on random variable \\(X\\) with range=\\(\\{x_1,\\ldots,x_m\\}\\), then,\n\\[\nP(X = x_i) = P({s_j \\in S: X(s_j)= x_i})\n\\]\n\n\nExample 1.4.3 (Three coin tosses-II)\nConsider experiment of tossing a fair coin three times. Define the random variable \\(X\\) to be the number of heads obtained in three tosses.\nThe complete enumeration of X for each point in the sample space is\n\\[\\begin{array}{c c c c c c c c c}\n\\hline\n  s & HHH & HHT & HTH & THH & TTH & THT & HTT & TTT\\\\\n\\hline\nX(s) & 3 & 2 & 2 & 2 & 1 & 1 & 1 & 0 \\\\\n\\hline\n\\end{array}\\]\nThe range for the random variable \\(X\\) is \\({\\it{X}}=\\{0,1,2,3\\}\\).\nThe probability function is:\n\\[\\begin{array}{c c c c c}\n\\hline\n  x & 0 & 1 & 2 & 3 \\\\\n\\hline\nP(X=x) & \\frac{1}{8} & \\frac{3}{8} & \\frac{3}{8} & \\frac{1}{8} \\\\\n\\hline\n\\end{array}\\]\nFor example, \\(P(X=1) = P({HTT, THT, TTH}) = \\frac{3}{8}\\)\n\n\n\n\nDefinition 1.5.1 the cumulative distribution function or cdf of a random variable \\(X\\), is defined by:\n\\[\nF(X) = P(X \\le x), \\text{ for all } x\n\\]\n\n\n\nExample 1.5.2 (Three coin tosses-III) Experiment of tossing three fair coins. Let \\(X\\) be the number of heads observed. The \\(cdf\\) of \\(X\\) is\n\\[\\begin{align*}\n  F(x) =\n  \\begin{dcases*}\n    0 & if  $-\\infty &lt; x &lt; 0$\\\\\n    \\tfrac{1}{8} & if  $0 \\le x \\lt 1 $\\\\\n    \\tfrac{1}{2} & if  $1 \\le x \\lt 2 $\\\\\n    \\tfrac{7}{8} & if  $2 \\le x \\lt 3 $\\\\\n    1 & if  $3 \\le x \\lt \\infty $\\\\\n  \\end{dcases*}\n\\end{align*}\\]\n\n\n\n\nlibrary(DiscreteDists)\nx &lt;- 0:3\nfx &lt;- c(0.125,0.375,0.375,0.125)\nplot_discrete_cdf(x, fx, las=1, main=\"Discrete CDF\")\n\n\n\n\n\n\n\n\nTheorem 1.5.3 The function \\(F(x)\\) is a cdf if and only if the following three conditions hold:\n\n\\(lim_{x \\rightarrow -\\infty} F(x) = 0\\) and \\(lim_{x \\rightarrow \\infty} F(x) = 1\\)\n\\(F(x)\\) is a nondecreasing function of \\(x\\)\n\\(F(x)\\) is right-continuous; that is, for every number \\(x_0\\), \\(lim_{x|x_0} F(x) = F(x_0)\\)\n\n\n\n\nExample 1.5.4 (Tossing for a head)\nLet \\(X\\) be a random variable for number of tosses required to get a head. If \\(p\\) is the probability of a head on any given toss, then, for \\(x = 1,2,\\ldots\\),\n\\[\nP(X = x) = (1-p)^{x-1}p\n\\]\nthen,\n\\[\nP(X \\le x) = \\sum_{i=1}^x P(X=i) = \\sum_{i=1}^x (1-p)^{i-1} p\n\\]\nNote that the partial sum of the geometric series is:\n\\[\n\\sum_{k=1}^n t^{k-1} = \\frac{1-t^n}{1-t}, \\quad t \\ne 1.\n\\]\nwhich can be induced by induction. Thus, the geometric cdf is given by:\n\\[\\begin{align*}\n  F_X (x) &= P(X \\le x) \\\\\n          &= \\frac{1-(1-p)^x}{1-(1-p)} . p \\\\\n          &= 1 - (1-p)^x, \\quad x = 1,2,\\ldots.\n\\end{align*}\\]\nProof that \\(F_X(x)\\) is a CDF:\nWe verify CDF using Theorem 1.5.3. above. Noting that for \\(0 \\lt p \\lt 1\\),\n\n\\(lim_{x \\pm \\infty} F_X(x)\\)\n\n\nIf \\(x \\lt 0\\) i.e. negative number of heads then \\(P(X=x)=0\\) and \\(P(X \\lt x) = 0\\). Then, \\(lim_{x \\rightarrow -\\infty} F_X(x) = 0\\).\nIf \\(x \\gt 0\\), then \\(lim_{x \\rightarrow \\infty} F_X (x) = lim_{x \\rightarrow \\infty} \\{1 - (1-p)^x\\} = 1\\) since \\((1-p)^x \\rightarrow 0\\) when \\(x \\rightarrow \\infty\\)\n\n\nSince \\(F_X(x) = \\sum_{i=1}^x (1-p)^{i-1} p\\), and \\(0 \\lt p \\lt 0\\), the sum contains positive additions, which means \\(F_X(x)\\) is non-decreasing function of x.\nSince \\((1-p)^\\epsilon  \\rightarrow 1\\) when \\(\\epsilon \\rightarrow 0\\) and,\n\n\\[\\begin{align*}\n  F_X(x) &= 1 - (1-p)^x \\qquad x=1,2,\\ldots \\\\\n  F_X(x+\\epsilon) &= 1 - (1-p)^{x+\\epsilon} \\\\\n  &= 1 -(1-p)^x(1-p)^\\epsilon \\\\\n  lim_{x \\downarrow 0} F_X(x +\\epsilon) &= 1 - (1-p)^x.1 \\\\\n  &= F_X(x)\n\\end{align*}\\]\nand we showed that \\(F_X(x)\\) is right-continuous. \\(\\qquad \\blacksquare\\)\nPlot of Geometric CDF\n\\[\nf(x) = P(X = x) = (1-p)^{x-1}p \\qquad x = 1,2,3,\\ldots\n\\]\n\nx &lt;- 0:15\np &lt;- 0.3\nfx &lt;- (1-p)^{x-1} * p\nplot_discrete_cdf(x,fx,main=\"CDF for Geometric Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  f(x) &= \\frac{x^{-x}}{(1+ e^{-x})^2}  \\\\\n  F_X(x) &= \\frac{1}{1+e^{-x}} \\qquad x \\in \\mathbb{R} \\tag{1.5.5}\\\\               \n\\end{align*}\\]\nProof (1.5.5) is a continuous CDF\n\nCondition 1\n\n\nSince \\(lim_{x \\rightarrow -\\infty} e^{-x} = \\infty, \\quad\\) \\(lim_{x \\rightarrow -\\infty} F_X(x) = 0\\)\nSince \\(lim_{x \\rightarrow \\infty} e^{-x} = 0, \\quad\\) \\(lim_{x \\rightarrow \\infty} F_X(x) = 1\\)\n\n\nCondition 2\n\n\nSince \\({d \\over dx} {F_X(x)} = \\dfrac{e^{-x}}{(1+e^{-x})^2} \\gt 0\\), then \\(F_X(x)\\) is an increasing function.\n\n\nCondition 3\n\n\n\\(F_X(x)\\) is continuous, and thus right continuous."
  },
  {
    "objectID": "prob2.html#random-variables",
    "href": "prob2.html#random-variables",
    "title": "Probability Theory 2",
    "section": "",
    "text": "Definition 1.4.1 A random variable is a function from a sample space \\(S\\) into the real numbers.\n\n\nSuppose \\(S = \\{s_1,\\ldots, s_n\\}\\) and there is a probability function \\(P\\) on random variable \\(X\\) with range=\\(\\{x_1,\\ldots,x_m\\}\\), then,\n\\[\nP(X = x_i) = P({s_j \\in S: X(s_j)= x_i})\n\\]\n\n\nExample 1.4.3 (Three coin tosses-II)\nConsider experiment of tossing a fair coin three times. Define the random variable \\(X\\) to be the number of heads obtained in three tosses.\nThe complete enumeration of X for each point in the sample space is\n\\[\\begin{array}{c c c c c c c c c}\n\\hline\n  s & HHH & HHT & HTH & THH & TTH & THT & HTT & TTT\\\\\n\\hline\nX(s) & 3 & 2 & 2 & 2 & 1 & 1 & 1 & 0 \\\\\n\\hline\n\\end{array}\\]\nThe range for the random variable \\(X\\) is \\({\\it{X}}=\\{0,1,2,3\\}\\).\nThe probability function is:\n\\[\\begin{array}{c c c c c}\n\\hline\n  x & 0 & 1 & 2 & 3 \\\\\n\\hline\nP(X=x) & \\frac{1}{8} & \\frac{3}{8} & \\frac{3}{8} & \\frac{1}{8} \\\\\n\\hline\n\\end{array}\\]\nFor example, \\(P(X=1) = P({HTT, THT, TTH}) = \\frac{3}{8}\\)\n\n\n\n\nDefinition 1.5.1 the cumulative distribution function or cdf of a random variable \\(X\\), is defined by:\n\\[\nF(X) = P(X \\le x), \\text{ for all } x\n\\]\n\n\n\nExample 1.5.2 (Three coin tosses-III) Experiment of tossing three fair coins. Let \\(X\\) be the number of heads observed. The \\(cdf\\) of \\(X\\) is\n\\[\\begin{align*}\n  F(x) =\n  \\begin{dcases*}\n    0 & if  $-\\infty &lt; x &lt; 0$\\\\\n    \\tfrac{1}{8} & if  $0 \\le x \\lt 1 $\\\\\n    \\tfrac{1}{2} & if  $1 \\le x \\lt 2 $\\\\\n    \\tfrac{7}{8} & if  $2 \\le x \\lt 3 $\\\\\n    1 & if  $3 \\le x \\lt \\infty $\\\\\n  \\end{dcases*}\n\\end{align*}\\]\n\n\n\n\nlibrary(DiscreteDists)\nx &lt;- 0:3\nfx &lt;- c(0.125,0.375,0.375,0.125)\nplot_discrete_cdf(x, fx, las=1, main=\"Discrete CDF\")\n\n\n\n\n\n\n\n\nTheorem 1.5.3 The function \\(F(x)\\) is a cdf if and only if the following three conditions hold:\n\n\\(lim_{x \\rightarrow -\\infty} F(x) = 0\\) and \\(lim_{x \\rightarrow \\infty} F(x) = 1\\)\n\\(F(x)\\) is a nondecreasing function of \\(x\\)\n\\(F(x)\\) is right-continuous; that is, for every number \\(x_0\\), \\(lim_{x|x_0} F(x) = F(x_0)\\)\n\n\n\n\nExample 1.5.4 (Tossing for a head)\nLet \\(X\\) be a random variable for number of tosses required to get a head. If \\(p\\) is the probability of a head on any given toss, then, for \\(x = 1,2,\\ldots\\),\n\\[\nP(X = x) = (1-p)^{x-1}p\n\\]\nthen,\n\\[\nP(X \\le x) = \\sum_{i=1}^x P(X=i) = \\sum_{i=1}^x (1-p)^{i-1} p\n\\]\nNote that the partial sum of the geometric series is:\n\\[\n\\sum_{k=1}^n t^{k-1} = \\frac{1-t^n}{1-t}, \\quad t \\ne 1.\n\\]\nwhich can be induced by induction. Thus, the geometric cdf is given by:\n\\[\\begin{align*}\n  F_X (x) &= P(X \\le x) \\\\\n          &= \\frac{1-(1-p)^x}{1-(1-p)} . p \\\\\n          &= 1 - (1-p)^x, \\quad x = 1,2,\\ldots.\n\\end{align*}\\]\nProof that \\(F_X(x)\\) is a CDF:\nWe verify CDF using Theorem 1.5.3. above. Noting that for \\(0 \\lt p \\lt 1\\),\n\n\\(lim_{x \\pm \\infty} F_X(x)\\)\n\n\nIf \\(x \\lt 0\\) i.e. negative number of heads then \\(P(X=x)=0\\) and \\(P(X \\lt x) = 0\\). Then, \\(lim_{x \\rightarrow -\\infty} F_X(x) = 0\\).\nIf \\(x \\gt 0\\), then \\(lim_{x \\rightarrow \\infty} F_X (x) = lim_{x \\rightarrow \\infty} \\{1 - (1-p)^x\\} = 1\\) since \\((1-p)^x \\rightarrow 0\\) when \\(x \\rightarrow \\infty\\)\n\n\nSince \\(F_X(x) = \\sum_{i=1}^x (1-p)^{i-1} p\\), and \\(0 \\lt p \\lt 0\\), the sum contains positive additions, which means \\(F_X(x)\\) is non-decreasing function of x.\nSince \\((1-p)^\\epsilon  \\rightarrow 1\\) when \\(\\epsilon \\rightarrow 0\\) and,\n\n\\[\\begin{align*}\n  F_X(x) &= 1 - (1-p)^x \\qquad x=1,2,\\ldots \\\\\n  F_X(x+\\epsilon) &= 1 - (1-p)^{x+\\epsilon} \\\\\n  &= 1 -(1-p)^x(1-p)^\\epsilon \\\\\n  lim_{x \\downarrow 0} F_X(x +\\epsilon) &= 1 - (1-p)^x.1 \\\\\n  &= F_X(x)\n\\end{align*}\\]\nand we showed that \\(F_X(x)\\) is right-continuous. \\(\\qquad \\blacksquare\\)\nPlot of Geometric CDF\n\\[\nf(x) = P(X = x) = (1-p)^{x-1}p \\qquad x = 1,2,3,\\ldots\n\\]\n\nx &lt;- 0:15\np &lt;- 0.3\nfx &lt;- (1-p)^{x-1} * p\nplot_discrete_cdf(x,fx,main=\"CDF for Geometric Distribution\")\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  f(x) &= \\frac{x^{-x}}{(1+ e^{-x})^2}  \\\\\n  F_X(x) &= \\frac{1}{1+e^{-x}} \\qquad x \\in \\mathbb{R} \\tag{1.5.5}\\\\               \n\\end{align*}\\]\nProof (1.5.5) is a continuous CDF\n\nCondition 1\n\n\nSince \\(lim_{x \\rightarrow -\\infty} e^{-x} = \\infty, \\quad\\) \\(lim_{x \\rightarrow -\\infty} F_X(x) = 0\\)\nSince \\(lim_{x \\rightarrow \\infty} e^{-x} = 0, \\quad\\) \\(lim_{x \\rightarrow \\infty} F_X(x) = 1\\)\n\n\nCondition 2\n\n\nSince \\({d \\over dx} {F_X(x)} = \\dfrac{e^{-x}}{(1+e^{-x})^2} \\gt 0\\), then \\(F_X(x)\\) is an increasing function.\n\n\nCondition 3\n\n\n\\(F_X(x)\\) is continuous, and thus right continuous."
  }
]